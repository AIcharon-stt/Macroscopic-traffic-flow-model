{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9acd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_logging(output_dir):\n",
    "    \"\"\"Set up logging\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create log filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = os.path.join(output_dir, f\"analysis_log_{timestamp}.log\")\n",
    "    \n",
    "    # Configure logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, encoding='utf-8'),\n",
    "            logging.StreamHandler()  # Also output to console\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Logging initialized, log file: {log_file}\")\n",
    "    return log_file\n",
    "\n",
    "def load_model_data(models, base_dir=\"./prediction_result_trafficmodel/\"):\n",
    "    \"\"\"Load validation results for all models\"\"\"\n",
    "    model_results = {}\n",
    "    \n",
    "    logging.info(\"Starting to load model data...\")\n",
    "    logging.info(\"-\" * 60)\n",
    "    logging.info(f\"{'Model Name':<15} {'Status':<10} {'Records':<10}\")\n",
    "    logging.info(\"-\" * 60)\n",
    "    \n",
    "    print(\"Model data loading status:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model Name':<15} {'Status':<10} {'Records':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model in models:\n",
    "        file_path = os.path.join(base_dir, model, \"validation_results.csv\")\n",
    "        try:\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                model_results[model] = df\n",
    "                status_msg = f\"{model:<15} {'Success':<10} {len(df):<10}\"\n",
    "                print(status_msg)\n",
    "                logging.info(status_msg)\n",
    "                logging.info(f\"  Data preview: {df.head(2).to_dict()}\")\n",
    "            else:\n",
    "                status_msg = f\"{model:<15} {'File not found':<10} {0:<10}\"\n",
    "                print(status_msg)\n",
    "                logging.warning(status_msg)\n",
    "        except Exception as e:\n",
    "            status_msg = f\"{model:<15} {'Error':<10} {0:<10} - {str(e)}\"\n",
    "            print(status_msg)\n",
    "            logging.error(status_msg)\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    logging.info(\"-\" * 60)\n",
    "    \n",
    "    # Log total data loaded\n",
    "    total_records = sum(len(df) for df in model_results.values())\n",
    "    logging.info(f\"Loaded {len(model_results)} models, total records: {total_records}\")\n",
    "    \n",
    "    return model_results\n",
    "\n",
    "def calculate_metrics_with_stats(df):\n",
    "    \"\"\"Calculate evaluation metrics and their statistical properties\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        logging.warning(\"Received empty dataset for metrics calculation\")\n",
    "        return None\n",
    "    \n",
    "    logging.info(f\"Calculating metrics, dataset size: {len(df)}\")\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = np.abs(df['predicted'] - df['actual'])\n",
    "    squared_errors = np.square(df['predicted'] - df['actual'])\n",
    "    \n",
    "    # Calculate MAE and RMSE\n",
    "    mae = np.mean(errors)\n",
    "    rmse = np.sqrt(np.mean(squared_errors))\n",
    "    \n",
    "    # Calculate MAPE (handling zero actual values)\n",
    "    mask = df['actual'] > 0\n",
    "    if np.sum(mask) > 0:\n",
    "        percentage_errors = np.abs((df['predicted'][mask] - df['actual'][mask]) / df['actual'][mask]) * 100\n",
    "        mape = np.mean(percentage_errors)\n",
    "        mape_var = np.var(percentage_errors, ddof=1)\n",
    "        n_mape = sum(mask)\n",
    "        logging.info(f\"MAPE calculation: valid samples={n_mape}, mean={mape:.4f}, variance={mape_var:.4f}\")\n",
    "    else:\n",
    "        mape = np.nan\n",
    "        mape_var = np.nan\n",
    "        n_mape = 0\n",
    "        logging.warning(\"No valid MAPE samples (all actual values are 0)\")\n",
    "    \n",
    "    # Calculate variance - fix MAE and RMSE variance calculation\n",
    "    mae_var = np.var(errors, ddof=1)\n",
    "    \n",
    "    # Fix: Use Bootstrap method to estimate RMSE variance instead of direct calculation\n",
    "    n = len(errors)\n",
    "    bootstrap_rmse_samples = []\n",
    "    for _ in range(500):  # 500 bootstrap samples\n",
    "        indices = np.random.randint(0, n, size=n)\n",
    "        bootstrap_squared_errors = squared_errors[indices]\n",
    "        bootstrap_rmse = np.sqrt(np.mean(bootstrap_squared_errors))\n",
    "        bootstrap_rmse_samples.append(bootstrap_rmse)\n",
    "    \n",
    "    rmse_var = np.var(bootstrap_rmse_samples, ddof=1)\n",
    "    \n",
    "    logging.info(f\"MAE calculation: mean={mae:.4f}, variance={mae_var:.4f}\")\n",
    "    logging.info(f\"RMSE calculation: mean={rmse:.4f}, variance={rmse_var:.4f}\")\n",
    "    \n",
    "    # Sample size\n",
    "    n = len(errors)\n",
    "    \n",
    "    # Calculate 95% confidence interval half-width (t-distribution)\n",
    "    t_critical = stats.t.ppf(0.975, n-1)  # 95% confidence, two-tailed\n",
    "    \n",
    "    mae_ci_half = t_critical * np.sqrt(mae_var / n)\n",
    "    rmse_ci_half = t_critical * np.sqrt(rmse_var / n)\n",
    "    \n",
    "    if n_mape > 1:\n",
    "        mape_ci_half = t_critical * np.sqrt(mape_var / n_mape)\n",
    "    else:\n",
    "        mape_ci_half = np.nan\n",
    "    \n",
    "    logging.info(f\"Confidence interval half-width: MAE={mae_ci_half:.4f}, RMSE={rmse_ci_half:.4f}, MAPE={f'{mape_ci_half:.4f}' if not np.isnan(mape_ci_half) else 'N/A'}\")\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'MAE_var': mae_var,\n",
    "        'MAE_ci_half': mae_ci_half,\n",
    "        'RMSE': rmse,\n",
    "        'RMSE_var': rmse_var,\n",
    "        'RMSE_ci_half': rmse_ci_half,\n",
    "        'MAPE': mape,\n",
    "        'MAPE_var': mape_var,\n",
    "        'MAPE_ci_half': mape_ci_half,\n",
    "        'n_samples': n,\n",
    "        'n_mape_samples': n_mape\n",
    "    }\n",
    "\n",
    "def bootstrap_ci(data, metric_function, n_bootstraps=2000, alpha=0.05):\n",
    "    \"\"\"Calculate confidence interval using bootstrap method\"\"\"\n",
    "    bootstrap_samples = []\n",
    "    n = len(data)\n",
    "    \n",
    "    logging.info(f\"Starting Bootstrap analysis: samples={n}, repetitions={n_bootstraps}\")\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        # Random sampling with replacement\n",
    "        indices = np.random.randint(0, n, size=n)\n",
    "        sample = data.iloc[indices]\n",
    "        \n",
    "        # Calculate statistic\n",
    "        metric = metric_function(sample)\n",
    "        bootstrap_samples.append(metric)\n",
    "        \n",
    "        # Log progress\n",
    "        if i % 500 == 0 and i > 0:\n",
    "            logging.info(f\"  Bootstrap progress: {i}/{n_bootstraps} completed\")\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    ci_lower = np.percentile(bootstrap_samples, alpha/2 * 100)\n",
    "    ci_upper = np.percentile(bootstrap_samples, (1-alpha/2) * 100)\n",
    "    \n",
    "    # Return mean value and confidence interval half-width\n",
    "    mean_value = np.mean(bootstrap_samples)\n",
    "    ci_half = (ci_upper - ci_lower) / 2\n",
    "    \n",
    "    logging.info(f\"Bootstrap results: mean={mean_value:.6f}, confidence interval=[{ci_lower:.6f}, {ci_upper:.6f}], half-width={ci_half:.6f}\")\n",
    "    \n",
    "    return mean_value, ci_half\n",
    "\n",
    "def analyze_by_group(model_results, group_by):\n",
    "    \"\"\"Analyze by specified grouping variable\"\"\"\n",
    "    group_metrics = {}\n",
    "    \n",
    "    logging.info(f\"Starting analysis by '{group_by}' grouping...\")\n",
    "    \n",
    "    for model, df in model_results.items():\n",
    "        if group_by not in df.columns:\n",
    "            logging.warning(f\"Model {model} data does not have '{group_by}' column, skipping analysis\")\n",
    "            continue\n",
    "            \n",
    "        unique_values = df[group_by].unique()\n",
    "        logging.info(f\"Unique values of '{group_by}' in model {model}: {unique_values}\")\n",
    "        \n",
    "        model_group_results = []\n",
    "        for group_value in unique_values:\n",
    "            group_df = df[df[group_by] == group_value]\n",
    "            \n",
    "            logging.info(f\"Analyzing model {model}, {group_by}={group_value}, samples={len(group_df)}\")\n",
    "            \n",
    "            metrics = calculate_metrics_with_stats(group_df)\n",
    "            if metrics:\n",
    "                metrics['model'] = model\n",
    "                metrics[group_by] = group_value\n",
    "                model_group_results.append(metrics)\n",
    "        \n",
    "        if model_group_results:\n",
    "            group_metrics[model] = pd.DataFrame(model_group_results)\n",
    "            logging.info(f\"Completed analysis of model {model} by '{group_by}', {len(model_group_results)} groups\")\n",
    "    \n",
    "    return group_metrics\n",
    "\n",
    "def format_metrics_table(results_dict):\n",
    "    \"\"\"Format metrics table, adding ± symbols\"\"\"\n",
    "    formatted_rows = []\n",
    "    \n",
    "    logging.info(\"Formatting metrics table...\")\n",
    "    \n",
    "    for model, metrics in results_dict.items():\n",
    "        # Standard format: value ± 95% confidence interval half-width\n",
    "        mae_formatted = f\"{metrics['MAE']:.4f} ± {metrics['MAE_ci_half']:.4f}\"\n",
    "        rmse_formatted = f\"{metrics['RMSE']:.4f} ± {metrics['RMSE_ci_half']:.4f}\"\n",
    "        \n",
    "        if not np.isnan(metrics['MAPE']):\n",
    "            mape_formatted = f\"{metrics['MAPE']:.2f} ± {metrics['MAPE_ci_half']:.2f}\"\n",
    "        else:\n",
    "            mape_formatted = \"N/A\"\n",
    "        \n",
    "        row = {\n",
    "            'Model': model,\n",
    "            'MAE ± CI': mae_formatted,\n",
    "            'RMSE ± CI': rmse_formatted,\n",
    "            'MAPE(%) ± CI': mape_formatted,\n",
    "            'Samples': metrics['n_samples'],\n",
    "            'MAPE Samples': metrics['n_mape_samples']\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"Formatted row: {row}\")\n",
    "        formatted_rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(formatted_rows)\n",
    "\n",
    "def compare_bootstrap_vs_standard(overall_metrics, bootstrap_results):\n",
    "    \"\"\"Compare Bootstrap and standard confidence interval methods\"\"\"\n",
    "    comparison_rows = []\n",
    "    \n",
    "    logging.info(\"Comparing Bootstrap and standard confidence interval methods...\")\n",
    "    \n",
    "    for model in overall_metrics.keys():\n",
    "        if model in bootstrap_results:\n",
    "            std_metrics = overall_metrics[model]\n",
    "            boot_metrics = bootstrap_results[model]\n",
    "            \n",
    "            comparison_rows.append({\n",
    "                'Model': model,\n",
    "                'MAE_standard': f\"{std_metrics['MAE']:.4f} ± {std_metrics['MAE_ci_half']:.4f}\",\n",
    "                'MAE_bootstrap': f\"{boot_metrics['MAE_bootstrap']:.4f} ± {boot_metrics['MAE_ci_bootstrap']:.4f}\",\n",
    "                'RMSE_standard': f\"{std_metrics['RMSE']:.4f} ± {std_metrics['RMSE_ci_half']:.4f}\",\n",
    "                'RMSE_bootstrap': f\"{boot_metrics['RMSE_bootstrap']:.4f} ± {boot_metrics['RMSE_ci_bootstrap']:.4f}\",\n",
    "                'MAPE_standard': f\"{std_metrics['MAPE']:.2f} ± {std_metrics['MAPE_ci_half']:.2f}\" if not np.isnan(std_metrics['MAPE']) else \"N/A\",\n",
    "                'MAPE_bootstrap': f\"{boot_metrics['MAPE_bootstrap']:.2f} ± {boot_metrics['MAPE_ci_bootstrap']:.2f}\" if not np.isnan(boot_metrics['MAPE_bootstrap']) else \"N/A\"\n",
    "            })\n",
    "            \n",
    "            logging.info(f\"Model {model} comparison: \"\n",
    "                        f\"MAE standard={std_metrics['MAE']:.4f}±{std_metrics['MAE_ci_half']:.4f} vs Bootstrap={boot_metrics['MAE_bootstrap']:.4f}±{boot_metrics['MAE_ci_bootstrap']:.4f}, \"\n",
    "                        f\"RMSE standard={std_metrics['RMSE']:.4f}±{std_metrics['RMSE_ci_half']:.4f} vs Bootstrap={boot_metrics['RMSE_bootstrap']:.4f}±{boot_metrics['RMSE_ci_bootstrap']:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(comparison_rows)\n",
    "\n",
    "def main():\n",
    "    # Model list\n",
    "    models = [\"LWR\", \"CTM\", \"METANET\", \"PI-LWR\", \"ML-CTM\"]\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"./model_metrics\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up logging\n",
    "    log_file = setup_logging(output_dir)\n",
    "    \n",
    "    logging.info(\"=\" * 80)\n",
    "    logging.info(\"Starting traffic model performance metrics analysis\")\n",
    "    logging.info(\"=\" * 80)\n",
    "    \n",
    "    print(\"Starting traffic model performance metrics analysis...\\n\")\n",
    "    \n",
    "    # 1. Load all model data\n",
    "    model_results = load_model_data(models)\n",
    "    \n",
    "    if not model_results:\n",
    "        logging.error(\"No valid model data found, analysis terminated\")\n",
    "        print(\"No valid model data found, analysis terminated\")\n",
    "        return\n",
    "    \n",
    "    # 2. Calculate overall performance metrics\n",
    "    logging.info(\"\\nCalculating overall model performance metrics...\")\n",
    "    print(\"\\nCalculating overall model performance metrics...\")\n",
    "    overall_metrics = {}\n",
    "    bootstrap_results = {}\n",
    "    \n",
    "    for model, df in model_results.items():\n",
    "        logging.info(f\"Analyzing overall performance of model {model}...\")\n",
    "        \n",
    "        # Calculate standard metrics and statistics\n",
    "        metrics = calculate_metrics_with_stats(df)\n",
    "        if metrics:\n",
    "            overall_metrics[model] = metrics\n",
    "            \n",
    "            # Additionally calculate confidence intervals using bootstrap\n",
    "            logging.info(f\"Using Bootstrap method to calculate confidence intervals for {model} model...\")\n",
    "            print(f\"  Using Bootstrap method to calculate confidence intervals for {model} model...\")\n",
    "            \n",
    "            # MAE bootstrap confidence interval\n",
    "            mae_func = lambda x: np.mean(np.abs(x['predicted'] - x['actual']))\n",
    "            mae_bootstrap, mae_ci = bootstrap_ci(df, mae_func)\n",
    "            \n",
    "            # RMSE bootstrap confidence interval\n",
    "            rmse_func = lambda x: np.sqrt(np.mean(np.square(x['predicted'] - x['actual'])))\n",
    "            rmse_bootstrap, rmse_ci = bootstrap_ci(df, rmse_func)\n",
    "            \n",
    "            # MAPE bootstrap confidence interval\n",
    "            def mape_func(x):\n",
    "                mask = x['actual'] > 0\n",
    "                if sum(mask) > 0:\n",
    "                    return np.mean(np.abs((x['predicted'][mask] - x['actual'][mask]) / x['actual'][mask])) * 100\n",
    "                return np.nan\n",
    "                \n",
    "            mape_bootstrap, mape_ci = bootstrap_ci(df, mape_func)\n",
    "            \n",
    "            bootstrap_results[model] = {\n",
    "                'MAE_bootstrap': mae_bootstrap,\n",
    "                'MAE_ci_bootstrap': mae_ci,\n",
    "                'RMSE_bootstrap': rmse_bootstrap,\n",
    "                'RMSE_ci_bootstrap': rmse_ci,\n",
    "                'MAPE_bootstrap': mape_bootstrap,\n",
    "                'MAPE_ci_bootstrap': mape_ci\n",
    "            }\n",
    "    \n",
    "    # 3. Format and save overall metrics results\n",
    "    if overall_metrics:\n",
    "        logging.info(\"Saving overall metrics results...\")\n",
    "        \n",
    "        # Convert to DataFrame and save raw data\n",
    "        overall_df = pd.DataFrame(overall_metrics).T.reset_index().rename(columns={'index': 'model'})\n",
    "        overall_file = os.path.join(output_dir, \"overall_metrics_raw.csv\")\n",
    "        overall_df.to_csv(overall_file, index=False)\n",
    "        logging.info(f\"Saved raw metrics to: {overall_file}\")\n",
    "        \n",
    "        # Add bootstrap results\n",
    "        if bootstrap_results:\n",
    "            bootstrap_df = pd.DataFrame(bootstrap_results).T.reset_index().rename(columns={'index': 'model'})\n",
    "            bootstrap_file = os.path.join(output_dir, \"bootstrap_metrics.csv\")\n",
    "            bootstrap_df.to_csv(bootstrap_file, index=False)\n",
    "            logging.info(f\"Saved Bootstrap metrics to: {bootstrap_file}\")\n",
    "            \n",
    "            # Compare results from both methods\n",
    "            comparison_df = compare_bootstrap_vs_standard(overall_metrics, bootstrap_results)\n",
    "            comparison_file = os.path.join(output_dir, \"metrics_method_comparison.csv\")\n",
    "            comparison_df.to_csv(comparison_file, index=False)\n",
    "            logging.info(f\"Saved method comparison to: {comparison_file}\")\n",
    "        \n",
    "        # Create formatted table\n",
    "        formatted_df = format_metrics_table(overall_metrics)\n",
    "        formatted_file = os.path.join(output_dir, \"overall_metrics_formatted.csv\") \n",
    "        formatted_df.to_csv(formatted_file, index=False)\n",
    "        logging.info(f\"Saved formatted metrics to: {formatted_file}\")\n",
    "        \n",
    "        # Create another formatted table using bootstrap results\n",
    "        if bootstrap_results:\n",
    "            bootstrap_formatted_rows = []\n",
    "            for model, metrics in bootstrap_results.items():\n",
    "                std_metrics = overall_metrics[model]\n",
    "                bootstrap_formatted_rows.append({\n",
    "                    'Model': model,\n",
    "                    'MAE ± CI': f\"{metrics['MAE_bootstrap']:.4f} ± {metrics['MAE_ci_bootstrap']:.4f}\",\n",
    "                    'RMSE ± CI': f\"{metrics['RMSE_bootstrap']:.4f} ± {metrics['RMSE_ci_bootstrap']:.4f}\",\n",
    "                    'MAPE(%) ± CI': f\"{metrics['MAPE_bootstrap']:.2f} ± {metrics['MAPE_ci_bootstrap']:.2f}\" if not np.isnan(metrics['MAPE_bootstrap']) else \"N/A\",\n",
    "                    'Samples': std_metrics['n_samples'],\n",
    "                    'MAPE Samples': std_metrics['n_mape_samples']\n",
    "                })\n",
    "            \n",
    "            bootstrap_formatted_df = pd.DataFrame(bootstrap_formatted_rows)\n",
    "            bootstrap_formatted_file = os.path.join(output_dir, \"overall_metrics_bootstrap_formatted.csv\")\n",
    "            bootstrap_formatted_df.to_csv(bootstrap_formatted_file, index=False)\n",
    "            logging.info(f\"Saved Bootstrap formatted metrics to: {bootstrap_formatted_file}\")\n",
    "        \n",
    "        # Print formatted table\n",
    "        print(\"\\nOverall performance metrics (95% confidence interval):\")\n",
    "        print(formatted_df.to_string(index=False))\n",
    "        \n",
    "        logging.info(\"\\nOverall performance metrics (95% confidence interval):\")\n",
    "        logging.info(\"\\n\" + formatted_df.to_string(index=False))\n",
    "    \n",
    "    # 4. Analyze by prediction time\n",
    "    logging.info(\"\\nAnalyzing by prediction time...\")\n",
    "    print(\"\\nAnalyzing by prediction time...\")\n",
    "    time_group_metrics = analyze_by_group(model_results, 'predict_minutes')\n",
    "    \n",
    "    if time_group_metrics:\n",
    "        # Combine results from all models\n",
    "        combined_df = pd.concat(time_group_metrics.values())\n",
    "        combined_file = os.path.join(output_dir, \"metrics_by_predict_time.csv\")\n",
    "        combined_df.to_csv(combined_file, index=False)\n",
    "        logging.info(f\"Saved metrics by prediction time to: {combined_file}\")\n",
    "        \n",
    "        # Create formatted version\n",
    "        formatted_rows = []\n",
    "        for model, df in time_group_metrics.items():\n",
    "            for _, row in df.iterrows():\n",
    "                formatted_rows.append({\n",
    "                    'Model': model,\n",
    "                    'Predict_Minutes': row['predict_minutes'],\n",
    "                    'MAE ± CI': f\"{row['MAE']:.4f} ± {row['MAE_ci_half']:.4f}\",\n",
    "                    'RMSE ± CI': f\"{row['RMSE']:.4f} ± {row['RMSE_ci_half']:.4f}\",\n",
    "                    'MAPE(%) ± CI': f\"{row['MAPE']:.2f} ± {row['MAPE_ci_half']:.2f}\" if not np.isnan(row['MAPE']) else \"N/A\"\n",
    "                })\n",
    "        \n",
    "        formatted_time_df = pd.DataFrame(formatted_rows)\n",
    "        formatted_time_file = os.path.join(output_dir, \"formatted_metrics_by_predict_time.csv\")\n",
    "        formatted_time_df.to_csv(formatted_time_file, index=False)\n",
    "        logging.info(f\"Saved formatted metrics by prediction time to: {formatted_time_file}\")\n",
    "    \n",
    "    # 5. Analyze by vehicle type\n",
    "    logging.info(\"Analyzing by vehicle type...\")\n",
    "    print(\"Analyzing by vehicle type...\")\n",
    "    vehicle_group_metrics = analyze_by_group(model_results, 'vehicle_type')\n",
    "    \n",
    "    if vehicle_group_metrics:\n",
    "        # Combine results from all models\n",
    "        combined_df = pd.concat(vehicle_group_metrics.values())\n",
    "        combined_file = os.path.join(output_dir, \"metrics_by_vehicle_type.csv\")\n",
    "        combined_df.to_csv(combined_file, index=False)\n",
    "        logging.info(f\"Saved metrics by vehicle type to: {combined_file}\")\n",
    "        \n",
    "        # Create formatted version\n",
    "        formatted_rows = []\n",
    "        for model, df in vehicle_group_metrics.items():\n",
    "            for _, row in df.iterrows():\n",
    "                formatted_rows.append({\n",
    "                    'Model': model,\n",
    "                    'Vehicle_Type': row['vehicle_type'],\n",
    "                    'MAE ± CI': f\"{row['MAE']:.4f} ± {row['MAE_ci_half']:.4f}\",\n",
    "                    'RMSE ± CI': f\"{row['RMSE']:.4f} ± {row['RMSE_ci_half']:.4f}\",\n",
    "                    'MAPE(%) ± CI': f\"{row['MAPE']:.2f} ± {row['MAPE_ci_half']:.2f}\" if not np.isnan(row['MAPE']) else \"N/A\"\n",
    "                })\n",
    "        \n",
    "        formatted_vehicle_df = pd.DataFrame(formatted_rows)\n",
    "        formatted_vehicle_file = os.path.join(output_dir, \"formatted_metrics_by_vehicle_type.csv\")\n",
    "        formatted_vehicle_df.to_csv(formatted_vehicle_file, index=False)\n",
    "        logging.info(f\"Saved formatted metrics by vehicle type to: {formatted_vehicle_file}\")\n",
    "    \n",
    "    # 6. Analyze by segment type (if the column exists in the data)\n",
    "    has_segment_type = any('segment_type' in df.columns for df in model_results.values())\n",
    "    logging.info(f\"segment_type column exists in data: {has_segment_type}\")\n",
    "    \n",
    "    if has_segment_type:\n",
    "        logging.info(\"Analyzing by segment type...\")\n",
    "        print(\"Analyzing by segment type...\")\n",
    "        segment_group_metrics = analyze_by_group(model_results, 'segment_type')\n",
    "        \n",
    "        if segment_group_metrics:\n",
    "            # Combine results from all models\n",
    "            combined_df = pd.concat(segment_group_metrics.values())\n",
    "            combined_file = os.path.join(output_dir, \"metrics_by_segment_type.csv\")\n",
    "            combined_df.to_csv(combined_file, index=False)\n",
    "            logging.info(f\"Saved metrics by segment type to: {combined_file}\")\n",
    "            \n",
    "            # Create formatted version\n",
    "            formatted_rows = []\n",
    "            for model, df in segment_group_metrics.items():\n",
    "                for _, row in df.iterrows():\n",
    "                    formatted_rows.append({\n",
    "                        'Model': model,\n",
    "                        'Segment_Type': row['segment_type'],\n",
    "                        'MAE ± CI': f\"{row['MAE']:.4f} ± {row['MAE_ci_half']:.4f}\",\n",
    "                        'RMSE ± CI': f\"{row['RMSE']:.4f} ± {row['RMSE_ci_half']:.4f}\",\n",
    "                        'MAPE(%) ± CI': f\"{row['MAPE']:.2f} ± {row['MAPE_ci_half']:.2f}\" if not np.isnan(row['MAPE']) else \"N/A\"\n",
    "                    })\n",
    "            \n",
    "            formatted_segment_df = pd.DataFrame(formatted_rows)\n",
    "            formatted_segment_file = os.path.join(output_dir, \"formatted_metrics_by_segment_type.csv\")\n",
    "            formatted_segment_df.to_csv(formatted_segment_file, index=False)\n",
    "            logging.info(f\"Saved formatted metrics by segment type to: {formatted_segment_file}\")\n",
    "    \n",
    "    # 7. Create a results summary file\n",
    "    summary_msg = f\"\"\"\n",
    "Analysis Results Summary\n",
    "===================\n",
    "Analysis completed at: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "Models analyzed: {', '.join(model_results.keys())}\n",
    "Total samples: {sum(len(df) for df in model_results.values())}\n",
    "Output directory: {output_dir}\n",
    "Log file: {log_file}\n",
    "\n",
    "Files generated:\n",
    "{os.linesep.join([f\"- {file}\" for file in os.listdir(output_dir) if file.endswith('.csv')])}\n",
    "\n",
    "Notes:\n",
    "- Confidence intervals were calculated using both standard method (t-distribution) and Bootstrap method\n",
    "- Bootstrap method typically provides more accurate confidence interval estimates, especially for non-normal data\n",
    "- Please refer to the CSV files and log file for detailed results\n",
    "\"\"\"\n",
    "\n",
    "    summary_file = os.path.join(output_dir, \"analysis_summary.txt\")\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_msg)\n",
    "    \n",
    "    logging.info(\"\\n\" + summary_msg)\n",
    "    print(f\"\\nAnalysis complete! All results have been saved to: {output_dir}\")\n",
    "    print(f\"Log file: {log_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
